{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2iv2rJ1irpVc"
      },
      "outputs": [],
      "source": [
        "# Cartpole game, as per https://keon.io/deep-q-learning/\n",
        "# !pip install -q gym\n",
        "\n",
        "# GYM provides an environment for an agent to interact with. We'll look at the 'cartpole',\n",
        "#  or the unstable, top-heavy inverted pendulum on top of a cart, which our 'agent' must learn to balance.\n",
        "#\n",
        "# GYM provides:\n",
        "#  - a state (observation) of the system/environment\n",
        "#    (in case of cartpole: cart position and velocity, angle and velocity at the tip\n",
        "#  - a set of _actions_ the agent can undertake in the environment\n",
        "#    (in case of cartpole: moving the cart base left or right)\n",
        "\n",
        "# When an agent performs a certain action in the current enviroment, GYM returns\n",
        "#  - a _reward_ for a certain result of performing the action in the environment\n",
        "#    (in case of cartpole: +1 if it didn't die.)\n",
        "#  - the new _state_ of the environment after performing said action\n",
        "#    (in case of cartpole: the effect of gravity pulling on the pendulum's top, on the angle, in one time step)\n",
        "#  - a 'terminal' in case the state is such that the agent cannot continue (is 'dead')\n",
        "#    (in case of cartpole: if the angle exceeds > 5 degrees of tipping over, or it flies off-screen)\n",
        "\n",
        "# It is up to us to make an agent that learns which action to take in which state, to maximise the total reward before it dies.\n",
        "#  We do this by 'remembering' what the results were of certain actions on certain states in a memory (i.e. a list or deque),\n",
        "#  And after every 'death' we sample ('replay') from this memory to train a neural network that decides the best actions\n",
        "#   we can take on any state we've encountered so far, which maximises the total reward.\n",
        "\n",
        "# In addition, GYM provides a nice real-time visual output of the environment and the result of actions taken.\n",
        "#  In google COLAB however, we pull some strings, and only have a combined video at the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fA7nN0_rrwRa"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizer_v2.adam import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iVzZ7nmErzCw"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "### STUDENT CODE HERE ###\n",
        "### --> Change the parameters below to perform a better training\n",
        "\n",
        "MAXRUNS = 500\n",
        "NSTEPSOLVED = 1000\n",
        "\n",
        "GAMMA = 0.95  # 'future discount factor'\n",
        "LEARNING_RATE = 0.001 # rate at which to update weights after each training step\n",
        "\n",
        "MEMORY_SIZE = 1000 # size of container to hold actions and outcomes\n",
        "BATCH_SIZE = 32 # number of actions in memory to 'replay' after each death\n",
        "\n",
        "# probability to do 'random' actions, to sample from event space\n",
        "EXPLORATION_MAX = 1.0  \n",
        "EXPLORATION_MIN = 0.01\n",
        "EXPLORATION_DECAY = 0.999   # 0.995 --> 1% after ~35 runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U2bZXXrqr5Tj"
      },
      "outputs": [],
      "source": [
        "class DQNSolver:\n",
        "\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        self.exploration_rate = EXPLORATION_MAX\n",
        "\n",
        "        self.action_space = action_space\n",
        "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
        "\n",
        "        # This will be a simple feed-forward NN, with \n",
        "        #  - input = 'observation' (aka state)\n",
        "        #  - output = predicted 'quality' of each possible action\n",
        "        self.model = Sequential()\n",
        "\n",
        "        ### STUDENT CODE HERE ###\n",
        "        ### --> Write the network using fully-connected (Dense) layers.\n",
        "        ###  Make sure the input has the same shape as an observation/state,\n",
        "        ###   and the output has the same dimensions as the number of possible actions.\n",
        "\n",
        "        self.model.add(Dense(24,activation='relu',input_shape=(4,)))\n",
        "        self.model.add(Dense(24,activation='relu'))\n",
        "        self.model.add(Dense(2,activation = 'linear'))\n",
        "\n",
        "        ### END STUDENT CODE ###\n",
        "        \n",
        "        self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
        "        self.model.summary()\n",
        "        \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # add event to memory\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        # return the best possible action for the current state\n",
        "\n",
        "        # sometimes allow for a random action at the 'exploration rate', to avoid local minima\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return random.randrange(self.action_space)\n",
        "        \n",
        "        # Get predicted qualities for each possible action, and return the action (=index) with the highest quality\n",
        "        q_values = self.model.predict(state) \n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def experience_replay(self):\n",
        "        # Learn from random subset of memory (reduces corr. between subsequent actions).\n",
        "        # learning is done by comparing 'predicted quality' to the here defined quality (~reward) of the action.\n",
        "        \n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            # We haven't experienced enough to properly learn yet - keep exploring!\n",
        "            return\n",
        "        \n",
        "        # Get random subset of memory\n",
        "        batch = random.sample(self.memory, BATCH_SIZE) \n",
        "        \n",
        "        for state, action, reward, state_next, terminal in batch:\n",
        "\n",
        "            # We define the 'quality' of a move by taking the known, memorized reward for the action,\n",
        "            #  and adding the predicted quality of the (predicted) best choice of action for the next state, to that.\n",
        "            # As the model learns to give this situation a low quality, any step leading up to this state will get a \n",
        "            #  lower quality due to the predict(state_next) term. This will slowly trickle through to the step before that, etc.,\n",
        "            #  slowly making our agent learn about future consequences of current actions.\n",
        "          \n",
        "            q_update = reward\n",
        "            if not terminal:\n",
        "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
        "                \n",
        "            # - Define the quality of the non-chosen action to just be the predicted quality (i.e. diff = 0)\n",
        "            # - Define the quality of the chosen action to be the newly defined quality\n",
        "            q_values = self.model.predict(state)\n",
        "            q_values[0][action] = q_update \n",
        "            \n",
        "            # Finally, find the optimal model weights for minimal difference between \n",
        "            #  predicted quality and observed quality (+ future prediction as per above) for this action.\n",
        "            # The weights are then updated * learning rate\n",
        "            self.model.fit(state, q_values, verbose=0) \n",
        "            \n",
        "        # reduce the 'random choices' rate over time, because you expect the model to have learned\n",
        "        self.exploration_rate *= EXPLORATION_DECAY\n",
        "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MCAlTnFAr8M8"
      },
      "outputs": [],
      "source": [
        "def cartpole() :\n",
        "    env = gym.make(ENV_NAME)\n",
        "\n",
        "    observation_space = env.observation_space.shape[0]\n",
        "    action_space = env.action_space.n\n",
        "    dqn_solver = DQNSolver(observation_space, action_space)\n",
        "       \n",
        "    run=0\n",
        "    runsteplog = []\n",
        "    #while True:\n",
        "    for i in range(MAXRUNS):\n",
        "        run += 1\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, observation_space])\n",
        "        step = 0\n",
        "        \n",
        "        while True :    \n",
        "            step += 1\n",
        "            # screen = env.render() # graphical output\n",
        "\n",
        "            # decide on an action\n",
        "            ### STUDENT CODE HERE\n",
        "            ### --> Redefine the action to be the output of the 'act' of your solver.\n",
        "            action = dqn_solver.act(state)\n",
        "            ### END STUDENT CODE\n",
        "            \n",
        "            # make the action\n",
        "            state_next, reward, terminal, info = env.step(action) \n",
        "\n",
        "            # if action made terminal: reduce reward!\n",
        "            reward = reward if not terminal else -reward  \n",
        "\n",
        "            state_next = np.reshape(state_next, [1, observation_space])\n",
        "\n",
        "            # fill agent memory with this action's results\n",
        "            dqn_solver.remember(state, action, reward, state_next, terminal) \n",
        "\n",
        "            # prepare for the next action in the environment\n",
        "            state = state_next \n",
        "            \n",
        "            if terminal :\n",
        "                #state = env.reset()\n",
        "                #state = np.reshape(state, [1, observation_space])\n",
        "                print(\"Run: {0}, exploration: {1:.15f}, score: {2}\".format(run,dqn_solver.exploration_rate,step))\n",
        "                runsteplog += [step]\n",
        "\n",
        "                break\n",
        "            \n",
        "            dqn_solver.experience_replay() # learn from batch of memories every time a new one is made\n",
        "            \n",
        "            if(step > NSTEPSOLVED) :\n",
        "               i = MAXRUNS\n",
        "               print(\"Solved! (step > NSTEPSOLVED)\")\n",
        "               break\n",
        "\n",
        "\n",
        "    env.close()\n",
        "    \n",
        "\n",
        "\n",
        "    # Show training process\n",
        "    \n",
        "    # plot #steps achieved\n",
        "    plt.plot(runsteplog)\n",
        "    plt.ylabel(\"# actions before terminal\")\n",
        "    plt.xlabel(\"run iteration\")\n",
        "\n",
        "    success_measure = np.mean(runsteplog[-15:])\n",
        "    print(\"Mean of last 15 runs: {0}\".format(success_measure))\n",
        "    return success_measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RBDY7DTDr-uK",
        "outputId": "a8df20f5-2bd4-4689-c1c6-650980b0d462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 24)                120       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 50        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 770\n",
            "Trainable params: 770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Run: 1, exploration: 1.000000000000000, score: 13\n",
            "Run: 2, exploration: 1.000000000000000, score: 14\n",
            "Run: 3, exploration: 0.994014980014994, score: 11\n",
            "Run: 4, exploration: 0.928637310700702, score: 69\n",
            "Run: 5, exploration: 0.913890031855952, score: 17\n",
            "Run: 6, exploration: 0.893994159022939, score: 23\n",
            "Run: 7, exploration: 0.885980320398478, score: 10\n",
            "Run: 8, exploration: 0.869297341854547, score: 20\n",
            "Run: 9, exploration: 0.857205969570888, score: 15\n",
            "Run: 10, exploration: 0.845282780573503, score: 15\n",
            "Run: 11, exploration: 0.822754382068589, score: 28\n",
            "Run: 12, exploration: 0.814563763637142, score: 11\n",
            "Run: 13, exploration: 0.793647733264306, score: 27\n",
            "Run: 14, exploration: 0.778703374116990, score: 20\n",
            "Run: 15, exploration: 0.757191494352590, score: 29\n",
            "Run: 16, exploration: 0.735537611879845, score: 30\n",
            "Run: 17, exploration: 0.718805041673813, score: 24\n",
            "Run: 18, exploration: 0.708806921422224, score: 15\n",
            "Run: 19, exploration: 0.696156268072043, score: 19\n",
            "Run: 20, exploration: 0.676247848439852, score: 30\n",
            "Run: 21, exploration: 0.649718787855196, score: 41\n",
            "Run: 22, exploration: 0.643894654146267, score: 10\n",
            "Run: 23, exploration: 0.632402542800493, score: 19\n",
            "Run: 24, exploration: 0.626106899731254, score: 11\n",
            "Run: 25, exploration: 0.621115540508403, score: 9\n",
            "Run: 26, exploration: 0.616163972680443, score: 9\n",
            "Run: 27, exploration: 0.608201726042335, score: 14\n",
            "Run: 28, exploration: 0.600342369826805, score: 14\n",
            "Run: 29, exploration: 0.590808597892234, score: 17\n",
            "Run: 30, exploration: 0.585512540067173, score: 10\n",
            "Run: 31, exploration: 0.579683692592110, score: 11\n",
            "Run: 32, exploration: 0.571620661686087, score: 15\n",
            "Run: 33, exploration: 0.564234016420244, score: 14\n",
            "Run: 34, exploration: 0.556942823491835, score: 14\n",
            "Run: 35, exploration: 0.551950341308961, score: 10\n",
            "Run: 36, exploration: 0.514617063201871, score: 71\n",
            "Run: 37, exploration: 0.499900234647728, score: 30\n",
            "Run: 38, exploration: 0.481251275088280, score: 39\n",
            "Run: 39, exploration: 0.461447607577367, score: 43\n",
            "Run: 40, exploration: 0.435868242731066, score: 58\n",
            "Run: 41, exploration: 0.415015340979119, score: 50\n",
            "Run: 42, exploration: 0.399133510121224, score: 40\n",
            "Run: 43, exploration: 0.367695424770964, score: 83\n",
            "Run: 44, exploration: 0.345234337085559, score: 64\n",
            "Run: 45, exploration: 0.328060455638172, score: 52\n",
            "Run: 46, exploration: 0.311117728783339, score: 54\n",
            "Run: 47, exploration: 0.268029443956713, score: 150\n",
            "Run: 48, exploration: 0.235811509486527, score: 129\n",
            "Run: 49, exploration: 0.218983366001841, score: 75\n",
            "Run: 50, exploration: 0.194209161934183, score: 121\n",
            "Run: 51, exploration: 0.155683509130990, score: 222\n",
            "Run: 52, exploration: 0.142562589736049, score: 89\n",
            "Run: 53, exploration: 0.124426239202403, score: 137\n",
            "Run: 54, exploration: 0.113711899708400, score: 91\n",
            "Run: 55, exploration: 0.101049350340789, score: 119\n",
            "Run: 56, exploration: 0.084564956528391, score: 179\n",
            "Run: 57, exploration: 0.076284396926748, score: 104\n",
            "Run: 58, exploration: 0.068883549261577, score: 103\n",
            "Run: 59, exploration: 0.063015004033047, score: 90\n",
            "Run: 60, exploration: 0.059047389632254, score: 66\n",
            "Run: 61, exploration: 0.047523853465876, score: 218\n",
            "Run: 62, exploration: 0.039691684335671, score: 181\n",
            "Run: 63, exploration: 0.033349895707045, score: 175\n",
            "Run: 64, exploration: 0.028416631551157, score: 161\n",
            "Run: 65, exploration: 0.024628468847869, score: 144\n",
            "Run: 66, exploration: 0.021409462503400, score: 141\n",
            "Run: 67, exploration: 0.018911514247096, score: 125\n",
            "Run: 68, exploration: 0.015731717456461, score: 185\n",
            "Run: 69, exploration: 0.015450940705938, score: 19\n",
            "Run: 70, exploration: 0.015114565495880, score: 23\n",
            "Run: 71, exploration: 0.014391451218652, score: 50\n",
            "Run: 72, exploration: 0.013689229397105, score: 51\n",
            "Run: 73, exploration: 0.013125912557246, score: 43\n",
            "Run: 74, exploration: 0.012311777356064, score: 65\n",
            "Run: 75, exploration: 0.012225853031415, score: 8\n",
            "Run: 76, exploration: 0.011947730008373, score: 24\n",
            "Run: 77, exploration: 0.011757993386804, score: 17\n",
            "Run: 78, exploration: 0.011364748028143, score: 35\n",
            "Run: 79, exploration: 0.011150750218998, score: 20\n",
            "Run: 80, exploration: 0.010929841199158, score: 21\n",
            "Run: 81, exploration: 0.010469594127423, score: 44\n",
            "Run: 82, exploration: 0.010282734769790, score: 19\n",
            "Run: 83, exploration: 0.010139708479234, score: 15\n",
            "Run: 84, exploration: 0.010000000000000, score: 15\n",
            "Run: 85, exploration: 0.010000000000000, score: 17\n",
            "Run: 86, exploration: 0.010000000000000, score: 15\n",
            "Run: 87, exploration: 0.010000000000000, score: 15\n",
            "Run: 88, exploration: 0.010000000000000, score: 10\n",
            "Run: 89, exploration: 0.010000000000000, score: 11\n",
            "Run: 90, exploration: 0.010000000000000, score: 14\n",
            "Run: 91, exploration: 0.010000000000000, score: 11\n",
            "Run: 92, exploration: 0.010000000000000, score: 8\n",
            "Run: 93, exploration: 0.010000000000000, score: 9\n",
            "Run: 94, exploration: 0.010000000000000, score: 9\n",
            "Run: 95, exploration: 0.010000000000000, score: 17\n",
            "Run: 96, exploration: 0.010000000000000, score: 15\n",
            "Run: 97, exploration: 0.010000000000000, score: 16\n",
            "Run: 98, exploration: 0.010000000000000, score: 17\n",
            "Run: 99, exploration: 0.010000000000000, score: 9\n",
            "Run: 100, exploration: 0.010000000000000, score: 9\n",
            "Run: 101, exploration: 0.010000000000000, score: 9\n",
            "Run: 102, exploration: 0.010000000000000, score: 11\n",
            "Run: 103, exploration: 0.010000000000000, score: 11\n",
            "Run: 104, exploration: 0.010000000000000, score: 9\n",
            "Run: 105, exploration: 0.010000000000000, score: 20\n",
            "Run: 106, exploration: 0.010000000000000, score: 20\n",
            "Run: 107, exploration: 0.010000000000000, score: 16\n",
            "Run: 108, exploration: 0.010000000000000, score: 14\n",
            "Run: 109, exploration: 0.010000000000000, score: 17\n",
            "Run: 110, exploration: 0.010000000000000, score: 15\n",
            "Run: 111, exploration: 0.010000000000000, score: 15\n",
            "Run: 112, exploration: 0.010000000000000, score: 15\n",
            "Run: 113, exploration: 0.010000000000000, score: 13\n",
            "Run: 114, exploration: 0.010000000000000, score: 32\n",
            "Run: 115, exploration: 0.010000000000000, score: 21\n",
            "Run: 116, exploration: 0.010000000000000, score: 22\n",
            "Run: 117, exploration: 0.010000000000000, score: 13\n",
            "Run: 118, exploration: 0.010000000000000, score: 36\n",
            "Run: 119, exploration: 0.010000000000000, score: 100\n",
            "Run: 120, exploration: 0.010000000000000, score: 60\n",
            "Run: 121, exploration: 0.010000000000000, score: 51\n",
            "Run: 122, exploration: 0.010000000000000, score: 63\n",
            "Run: 123, exploration: 0.010000000000000, score: 88\n",
            "Run: 124, exploration: 0.010000000000000, score: 144\n",
            "Run: 125, exploration: 0.010000000000000, score: 132\n",
            "Run: 126, exploration: 0.010000000000000, score: 132\n",
            "Run: 127, exploration: 0.010000000000000, score: 206\n",
            "Run: 128, exploration: 0.010000000000000, score: 146\n",
            "Run: 129, exploration: 0.010000000000000, score: 204\n",
            "Run: 130, exploration: 0.010000000000000, score: 148\n",
            "Run: 131, exploration: 0.010000000000000, score: 190\n",
            "Run: 132, exploration: 0.010000000000000, score: 181\n",
            "Run: 133, exploration: 0.010000000000000, score: 428\n",
            "Run: 134, exploration: 0.010000000000000, score: 267\n",
            "Run: 135, exploration: 0.010000000000000, score: 235\n",
            "Run: 136, exploration: 0.010000000000000, score: 102\n",
            "Run: 137, exploration: 0.010000000000000, score: 202\n",
            "Run: 138, exploration: 0.010000000000000, score: 241\n",
            "Run: 139, exploration: 0.010000000000000, score: 188\n",
            "Run: 140, exploration: 0.010000000000000, score: 158\n",
            "Run: 141, exploration: 0.010000000000000, score: 159\n",
            "Run: 142, exploration: 0.010000000000000, score: 316\n",
            "Run: 143, exploration: 0.010000000000000, score: 155\n",
            "Run: 144, exploration: 0.010000000000000, score: 118\n",
            "Run: 145, exploration: 0.010000000000000, score: 164\n",
            "Run: 146, exploration: 0.010000000000000, score: 74\n",
            "Run: 147, exploration: 0.010000000000000, score: 145\n",
            "Run: 148, exploration: 0.010000000000000, score: 226\n",
            "Run: 149, exploration: 0.010000000000000, score: 154\n",
            "Run: 150, exploration: 0.010000000000000, score: 140\n",
            "Run: 151, exploration: 0.010000000000000, score: 161\n",
            "Run: 152, exploration: 0.010000000000000, score: 140\n",
            "Run: 153, exploration: 0.010000000000000, score: 193\n",
            "Run: 154, exploration: 0.010000000000000, score: 269\n",
            "Run: 155, exploration: 0.010000000000000, score: 199\n",
            "Run: 156, exploration: 0.010000000000000, score: 205\n",
            "Run: 157, exploration: 0.010000000000000, score: 219\n",
            "Run: 158, exploration: 0.010000000000000, score: 204\n",
            "Run: 159, exploration: 0.010000000000000, score: 197\n",
            "Run: 160, exploration: 0.010000000000000, score: 34\n",
            "Run: 161, exploration: 0.010000000000000, score: 217\n",
            "Run: 162, exploration: 0.010000000000000, score: 173\n",
            "Run: 163, exploration: 0.010000000000000, score: 286\n",
            "Run: 164, exploration: 0.010000000000000, score: 194\n",
            "Run: 165, exploration: 0.010000000000000, score: 304\n",
            "Run: 166, exploration: 0.010000000000000, score: 360\n",
            "Run: 167, exploration: 0.010000000000000, score: 10\n",
            "Run: 168, exploration: 0.010000000000000, score: 10\n",
            "Run: 169, exploration: 0.010000000000000, score: 8\n",
            "Run: 170, exploration: 0.010000000000000, score: 9\n",
            "Run: 171, exploration: 0.010000000000000, score: 10\n",
            "Run: 172, exploration: 0.010000000000000, score: 10\n",
            "Run: 173, exploration: 0.010000000000000, score: 10\n",
            "Run: 174, exploration: 0.010000000000000, score: 10\n",
            "Run: 175, exploration: 0.010000000000000, score: 10\n",
            "Run: 176, exploration: 0.010000000000000, score: 21\n",
            "Run: 177, exploration: 0.010000000000000, score: 154\n",
            "Run: 178, exploration: 0.010000000000000, score: 143\n",
            "Run: 179, exploration: 0.010000000000000, score: 149\n",
            "Run: 180, exploration: 0.010000000000000, score: 145\n",
            "Run: 181, exploration: 0.010000000000000, score: 166\n",
            "Run: 182, exploration: 0.010000000000000, score: 208\n",
            "Run: 183, exploration: 0.010000000000000, score: 225\n",
            "Run: 184, exploration: 0.010000000000000, score: 188\n",
            "Run: 185, exploration: 0.010000000000000, score: 203\n",
            "Run: 186, exploration: 0.010000000000000, score: 67\n",
            "Run: 187, exploration: 0.010000000000000, score: 118\n",
            "Run: 188, exploration: 0.010000000000000, score: 185\n",
            "Run: 189, exploration: 0.010000000000000, score: 145\n",
            "Run: 190, exploration: 0.010000000000000, score: 82\n",
            "Run: 191, exploration: 0.010000000000000, score: 13\n",
            "Run: 192, exploration: 0.010000000000000, score: 161\n",
            "Run: 193, exploration: 0.010000000000000, score: 159\n",
            "Run: 194, exploration: 0.010000000000000, score: 216\n",
            "Run: 195, exploration: 0.010000000000000, score: 176\n",
            "Run: 196, exploration: 0.010000000000000, score: 130\n",
            "Run: 197, exploration: 0.010000000000000, score: 273\n",
            "Run: 198, exploration: 0.010000000000000, score: 281\n",
            "Run: 199, exploration: 0.010000000000000, score: 198\n",
            "Run: 200, exploration: 0.010000000000000, score: 164\n",
            "Run: 201, exploration: 0.010000000000000, score: 184\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    cartpole()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of ML_gym_reinforcement_exercise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
