{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2iv2rJ1irpVc"
      },
      "outputs": [],
      "source": [
        "# Cartpole game, as per https://keon.io/deep-q-learning/\n",
        "# !pip install -q gym\n",
        "\n",
        "# GYM provides an environment for an agent to interact with. We'll look at the 'cartpole',\n",
        "#  or the unstable, top-heavy inverted pendulum on top of a cart, which our 'agent' must learn to balance.\n",
        "#\n",
        "# GYM provides:\n",
        "#  - a state (observation) of the system/environment\n",
        "#    (in case of cartpole: cart position and velocity, angle and velocity at the tip\n",
        "#  - a set of _actions_ the agent can undertake in the environment\n",
        "#    (in case of cartpole: moving the cart base left or right)\n",
        "\n",
        "# When an agent performs a certain action in the current enviroment, GYM returns\n",
        "#  - a _reward_ for a certain result of performing the action in the environment\n",
        "#    (in case of cartpole: +1 if it didn't die.)\n",
        "#  - the new _state_ of the environment after performing said action\n",
        "#    (in case of cartpole: the effect of gravity pulling on the pendulum's top, on the angle, in one time step)\n",
        "#  - a 'terminal' in case the state is such that the agent cannot continue (is 'dead')\n",
        "#    (in case of cartpole: if the angle exceeds > 5 degrees of tipping over, or it flies off-screen)\n",
        "\n",
        "# It is up to us to make an agent that learns which action to take in which state, to maximise the total reward before it dies.\n",
        "#  We do this by 'remembering' what the results were of certain actions on certain states in a memory (i.e. a list or deque),\n",
        "#  And after every 'death' we sample ('replay') from this memory to train a neural network that decides the best actions\n",
        "#   we can take on any state we've encountered so far, which maximises the total reward.\n",
        "\n",
        "# In addition, GYM provides a nice real-time visual output of the environment and the result of actions taken.\n",
        "#  In google COLAB however, we pull some strings, and only have a combined video at the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fA7nN0_rrwRa"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizer_v2.adam import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iVzZ7nmErzCw"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "### STUDENT CODE HERE ###\n",
        "### --> Change the parameters below to perform a better training\n",
        "\n",
        "MAXRUNS = 500\n",
        "NSTEPSOLVED = 1000\n",
        "\n",
        "GAMMA = 0.95  # 'future discount factor'\n",
        "LEARNING_RATE = 0.001 # rate at which to update weights after each training step\n",
        "\n",
        "MEMORY_SIZE = 1000 # size of container to hold actions and outcomes\n",
        "BATCH_SIZE = 32 # number of actions in memory to 'replay' after each death\n",
        "\n",
        "# probability to do 'random' actions, to sample from event space\n",
        "EXPLORATION_MAX = 1.0  \n",
        "EXPLORATION_MIN = 0.01\n",
        "EXPLORATION_DECAY = 0.999   # 0.995 --> 1% after ~35 runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U2bZXXrqr5Tj"
      },
      "outputs": [],
      "source": [
        "class DQNSolver:\n",
        "\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        self.exploration_rate = EXPLORATION_MAX\n",
        "\n",
        "        self.action_space = action_space\n",
        "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
        "\n",
        "        # This will be a simple feed-forward NN, with \n",
        "        #  - input = 'observation' (aka state)\n",
        "        #  - output = predicted 'quality' of each possible action\n",
        "        self.model = Sequential()\n",
        "\n",
        "        ### STUDENT CODE HERE ###\n",
        "        ### --> Write the network using fully-connected (Dense) layers.\n",
        "        ###  Make sure the input has the same shape as an observation/state,\n",
        "        ###   and the output has the same dimensions as the number of possible actions.\n",
        "\n",
        "        self.model.add(Dense(16,activation='relu',input_shape=(4,)))\n",
        "        self.model.add(Dense(16,activation='relu'))\n",
        "        self.model.add(Dense(2,activation = 'linear'))\n",
        "\n",
        "        ### END STUDENT CODE ###\n",
        "        \n",
        "        self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
        "        self.model.summary()\n",
        "        \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # add event to memory\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        # return the best possible action for the current state\n",
        "\n",
        "        # sometimes allow for a random action at the 'exploration rate', to avoid local minima\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return random.randrange(self.action_space)\n",
        "        \n",
        "        # Get predicted qualities for each possible action, and return the action (=index) with the highest quality\n",
        "        q_values = self.model.predict(state) \n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def experience_replay(self):\n",
        "        # Learn from random subset of memory (reduces corr. between subsequent actions).\n",
        "        # learning is done by comparing 'predicted quality' to the here defined quality (~reward) of the action.\n",
        "        \n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            # We haven't experienced enough to properly learn yet - keep exploring!\n",
        "            return\n",
        "        \n",
        "        # Get random subset of memory\n",
        "        batch = random.sample(self.memory, BATCH_SIZE) \n",
        "        \n",
        "        for state, action, reward, state_next, terminal in batch:\n",
        "\n",
        "            # We define the 'quality' of a move by taking the known, memorized reward for the action,\n",
        "            #  and adding the predicted quality of the (predicted) best choice of action for the next state, to that.\n",
        "            # As the model learns to give this situation a low quality, any step leading up to this state will get a \n",
        "            #  lower quality due to the predict(state_next) term. This will slowly trickle through to the step before that, etc.,\n",
        "            #  slowly making our agent learn about future consequences of current actions.\n",
        "          \n",
        "            q_update = reward\n",
        "            if not terminal:\n",
        "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
        "                \n",
        "            # - Define the quality of the non-chosen action to just be the predicted quality (i.e. diff = 0)\n",
        "            # - Define the quality of the chosen action to be the newly defined quality\n",
        "            q_values = self.model.predict(state)\n",
        "            q_values[0][action] = q_update \n",
        "            \n",
        "            # Finally, find the optimal model weights for minimal difference between \n",
        "            #  predicted quality and observed quality (+ future prediction as per above) for this action.\n",
        "            # The weights are then updated * learning rate\n",
        "            self.model.fit(state, q_values, verbose=0) \n",
        "            \n",
        "        # reduce the 'random choices' rate over time, because you expect the model to have learned\n",
        "        self.exploration_rate *= EXPLORATION_DECAY\n",
        "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MCAlTnFAr8M8"
      },
      "outputs": [],
      "source": [
        "def cartpole() :\n",
        "    env = gym.make(ENV_NAME)\n",
        "\n",
        "    observation_space = env.observation_space.shape[0]\n",
        "    action_space = env.action_space.n\n",
        "    dqn_solver = DQNSolver(observation_space, action_space)\n",
        "       \n",
        "    run=0\n",
        "    runsteplog = []\n",
        "    #while True:\n",
        "    for i in range(MAXRUNS):\n",
        "        run += 1\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, observation_space])\n",
        "        step = 0\n",
        "        \n",
        "        while True :    \n",
        "            step += 1\n",
        "            # screen = env.render() # graphical output\n",
        "\n",
        "            # decide on an action\n",
        "            ### STUDENT CODE HERE\n",
        "            ### --> Redefine the action to be the output of the 'act' of your solver.\n",
        "            action = dqn_solver.act(state)\n",
        "            ### END STUDENT CODE\n",
        "            \n",
        "            # make the action\n",
        "            state_next, reward, terminal, info = env.step(action) \n",
        "\n",
        "            # if action made terminal: reduce reward!\n",
        "            reward = reward if not terminal else -reward  \n",
        "\n",
        "            state_next = np.reshape(state_next, [1, observation_space])\n",
        "\n",
        "            # fill agent memory with this action's results\n",
        "            dqn_solver.remember(state, action, reward, state_next, terminal) \n",
        "\n",
        "            # prepare for the next action in the environment\n",
        "            state = state_next \n",
        "            \n",
        "            if terminal :\n",
        "                #state = env.reset()\n",
        "                #state = np.reshape(state, [1, observation_space])\n",
        "                print(\"Run: {0}, exploration: {1:.15f}, score: {2}\".format(run,dqn_solver.exploration_rate,step))\n",
        "                runsteplog += [step]\n",
        "\n",
        "                break\n",
        "            \n",
        "            dqn_solver.experience_replay() # learn from batch of memories every time a new one is made\n",
        "            \n",
        "            if(step > NSTEPSOLVED) :\n",
        "               i = MAXRUNS\n",
        "               print(\"Solved! (step > NSTEPSOLVED)\")\n",
        "               break\n",
        "\n",
        "\n",
        "    env.close()\n",
        "    \n",
        "\n",
        "\n",
        "    # Show training process\n",
        "    \n",
        "    # plot #steps achieved\n",
        "    plt.plot(runsteplog)\n",
        "    plt.ylabel(\"# actions before terminal\")\n",
        "    plt.xlabel(\"run iteration\")\n",
        "\n",
        "    success_measure = np.mean(runsteplog[-15:])\n",
        "    print(\"Mean of last 15 runs: {0}\".format(success_measure))\n",
        "    return success_measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RBDY7DTDr-uK",
        "outputId": "a8df20f5-2bd4-4689-c1c6-650980b0d462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                80        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 386\n",
            "Trainable params: 386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Run: 1, exploration: 1.000000000000000, score: 13\n",
            "Run: 2, exploration: 1.000000000000000, score: 12\n",
            "Run: 3, exploration: 0.989054835329539, score: 18\n",
            "Run: 4, exploration: 0.946459102605027, score: 45\n",
            "Run: 5, exploration: 0.935163851921285, score: 13\n",
            "Run: 6, exploration: 0.924003401238575, score: 13\n",
            "Run: 7, exploration: 0.912063165682272, score: 14\n",
            "Run: 8, exploration: 0.894889048071010, score: 20\n",
            "Run: 9, exploration: 0.885980320398478, score: 11\n",
            "Run: 10, exploration: 0.878038318495602, score: 10\n",
            "Run: 11, exploration: 0.866692056851711, score: 14\n",
            "Run: 12, exploration: 0.849521903362253, score: 21\n",
            "Run: 13, exploration: 0.836031021347021, score: 17\n",
            "Run: 14, exploration: 0.821931627686521, score: 18\n",
            "Run: 15, exploration: 0.808878894649479, score: 17\n",
            "Run: 16, exploration: 0.785746875008398, score: 30\n",
            "Run: 17, exploration: 0.774042818860509, score: 16\n",
            "Run: 18, exploration: 0.764805221953346, score: 13\n",
            "Run: 19, exploration: 0.755677868555380, score: 13\n",
            "Run: 20, exploration: 0.738487132440021, score: 24\n",
            "Run: 21, exploration: 0.715934131462867, score: 32\n",
            "Run: 22, exploration: 0.707390016386301, score: 13\n",
            "Run: 23, exploration: 0.694764651692167, score: 19\n",
            "Run: 24, exploration: 0.676247848439852, score: 28\n",
            "Run: 25, exploration: 0.654285066209362, score: 34\n",
            "Run: 26, exploration: 0.613703011309517, score: 65\n",
            "Run: 27, exploration: 0.604561626557057, score: 16\n",
            "Run: 28, exploration: 0.575062421773024, score: 51\n",
            "Run: 29, exploration: 0.562543006508798, score: 23\n",
            "Run: 30, exploration: 0.552502844153115, score: 19\n",
            "Run: 31, exploration: 0.529237381141090, score: 44\n",
            "Run: 32, exploration: 0.516680687935692, score: 25\n",
            "Run: 33, exploration: 0.481733008096377, score: 71\n",
            "Run: 34, exploration: 0.463761783512394, score: 39\n",
            "Run: 35, exploration: 0.457768921440970, score: 14\n",
            "Run: 36, exploration: 0.429375787529190, score: 65\n",
            "Run: 37, exploration: 0.399133510121224, score: 74\n",
            "Run: 38, exploration: 0.355397874123048, score: 117\n",
            "Run: 39, exploration: 0.318997701691401, score: 109\n",
            "Run: 40, exploration: 0.267761414512756, score: 176\n",
            "Run: 41, exploration: 0.215937446872941, score: 216\n",
            "Run: 42, exploration: 0.177485817186748, score: 197\n",
            "Run: 43, exploration: 0.142705295031080, score: 219\n",
            "Run: 44, exploration: 0.127705405625056, score: 112\n",
            "Run: 45, exploration: 0.126814145133977, score: 8\n",
            "Run: 46, exploration: 0.125426143432941, score: 12\n",
            "Run: 47, exploration: 0.124301812963200, score: 10\n",
            "Run: 48, exploration: 0.123434306264315, score: 8\n",
            "Run: 49, exploration: 0.122327830790017, score: 10\n",
            "Run: 50, exploration: 0.121352626481156, score: 9\n",
            "Run: 51, exploration: 0.120264811359034, score: 10\n",
            "Run: 52, exploration: 0.119186747502905, score: 10\n",
            "Run: 53, exploration: 0.117764346696025, score: 13\n",
            "Run: 54, exploration: 0.116942465202800, score: 8\n",
            "Run: 55, exploration: 0.116010193329605, score: 9\n",
            "Run: 56, exploration: 0.115200554134059, score: 8\n",
            "Run: 57, exploration: 0.113825725433834, score: 13\n",
            "Run: 58, exploration: 0.112579884125226, score: 12\n",
            "Run: 59, exploration: 0.111682390992380, score: 9\n",
            "Run: 60, exploration: 0.110681260672262, score: 10\n",
            "Run: 61, exploration: 0.109579415463733, score: 11\n",
            "Run: 62, exploration: 0.105386024111510, score: 40\n",
            "Run: 63, exploration: 0.104545880833046, score: 9\n",
            "Run: 64, exploration: 0.103608722788565, score: 10\n",
            "Run: 65, exploration: 0.102679965507397, score: 10\n",
            "Run: 66, exploration: 0.101759533684396, score: 10\n",
            "Run: 67, exploration: 0.100847352689457, score: 10\n",
            "Run: 68, exploration: 0.099943348561465, score: 10\n",
            "Run: 69, exploration: 0.099146594596896, score: 9\n",
            "Run: 70, exploration: 0.098454647046542, score: 8\n",
            "Run: 71, exploration: 0.097377044722045, score: 12\n",
            "Run: 72, exploration: 0.080519181779822, score: 191\n",
            "Run: 73, exploration: 0.070346138624348, score: 136\n",
            "Run: 74, exploration: 0.061581490692619, score: 134\n",
            "Run: 75, exploration: 0.051432552523262, score: 181\n",
            "Run: 76, exploration: 0.044353749128151, score: 149\n",
            "Run: 77, exploration: 0.038944291303457, score: 131\n",
            "Run: 78, exploration: 0.038518039622166, score: 12\n",
            "Run: 79, exploration: 0.033316545811338, score: 146\n",
            "Run: 80, exploration: 0.029165531981040, score: 134\n",
            "Run: 81, exploration: 0.028788646656255, score: 14\n",
            "Run: 82, exploration: 0.025151399750013, score: 136\n",
            "Run: 83, exploration: 0.023996069242763, score: 48\n",
            "Run: 84, exploration: 0.023804771236658, score: 9\n",
            "Run: 85, exploration: 0.023591383270688, score: 10\n",
            "Run: 86, exploration: 0.023403311443787, score: 9\n",
            "Run: 87, exploration: 0.023193522197072, score: 10\n",
            "Run: 88, exploration: 0.023008622140902, score: 9\n",
            "Run: 89, exploration: 0.022848044162484, score: 8\n",
            "Run: 90, exploration: 0.022643232378252, score: 10\n",
            "Run: 91, exploration: 0.022462719263295, score: 9\n",
            "Run: 92, exploration: 0.022261361563778, score: 10\n",
            "Run: 93, exploration: 0.022083892744312, score: 9\n",
            "Run: 94, exploration: 0.021864044946608, score: 11\n",
            "Run: 95, exploration: 0.021668053813878, score: 10\n",
            "Run: 96, exploration: 0.021495314876979, score: 9\n",
            "Run: 97, exploration: 0.021302629071521, score: 10\n",
            "Run: 98, exploration: 0.021111670517784, score: 10\n",
            "Run: 99, exploration: 0.020922423732540, score: 10\n",
            "Run: 100, exploration: 0.020755629000352, score: 9\n",
            "Run: 101, exploration: 0.020569573801132, score: 10\n",
            "Run: 102, exploration: 0.020426018026359, score: 8\n",
            "Run: 103, exploration: 0.020242917487556, score: 10\n",
            "Run: 104, exploration: 0.020081539817158, score: 9\n",
            "Run: 105, exploration: 0.019901527209915, score: 10\n",
            "Run: 106, exploration: 0.019703405122531, score: 11\n",
            "Run: 107, exploration: 0.018481300707052, score: 65\n",
            "Run: 108, exploration: 0.018024770305406, score: 26\n",
            "Run: 109, exploration: 0.017667679144469, score: 21\n",
            "Run: 110, exploration: 0.017214015809443, score: 27\n",
            "Run: 111, exploration: 0.016788790243559, score: 26\n",
            "Run: 112, exploration: 0.016049679833077, score: 46\n",
            "Run: 113, exploration: 0.015747464921382, score: 20\n",
            "Run: 114, exploration: 0.015160000031133, score: 39\n",
            "Run: 115, exploration: 0.014405857075728, score: 52\n",
            "Run: 116, exploration: 0.013965909926436, score: 32\n",
            "Run: 117, exploration: 0.013580098094712, score: 29\n",
            "Run: 118, exploration: 0.013099673858044, score: 37\n",
            "Run: 119, exploration: 0.012398305564103, score: 56\n",
            "Run: 120, exploration: 0.012152681056688, score: 21\n",
            "Run: 121, exploration: 0.011864346382892, score: 25\n",
            "Run: 122, exploration: 0.011444620434316, score: 37\n",
            "Run: 123, exploration: 0.011229118630547, score: 20\n",
            "Run: 124, exploration: 0.010585453660853, score: 60\n",
            "Run: 125, exploration: 0.010262179582985, score: 32\n",
            "Run: 126, exploration: 0.010089111232576, score: 18\n",
            "Run: 127, exploration: 0.010000000000000, score: 24\n",
            "Run: 128, exploration: 0.010000000000000, score: 14\n",
            "Run: 129, exploration: 0.010000000000000, score: 16\n",
            "Run: 130, exploration: 0.010000000000000, score: 22\n",
            "Run: 131, exploration: 0.010000000000000, score: 39\n",
            "Run: 132, exploration: 0.010000000000000, score: 29\n",
            "Run: 133, exploration: 0.010000000000000, score: 36\n",
            "Run: 134, exploration: 0.010000000000000, score: 29\n",
            "Run: 135, exploration: 0.010000000000000, score: 34\n",
            "Run: 136, exploration: 0.010000000000000, score: 35\n",
            "Run: 137, exploration: 0.010000000000000, score: 45\n",
            "Run: 138, exploration: 0.010000000000000, score: 199\n",
            "Run: 139, exploration: 0.010000000000000, score: 53\n",
            "Run: 140, exploration: 0.010000000000000, score: 124\n",
            "Run: 141, exploration: 0.010000000000000, score: 176\n",
            "Run: 142, exploration: 0.010000000000000, score: 152\n",
            "Run: 143, exploration: 0.010000000000000, score: 119\n",
            "Run: 144, exploration: 0.010000000000000, score: 136\n",
            "Run: 145, exploration: 0.010000000000000, score: 166\n",
            "Run: 146, exploration: 0.010000000000000, score: 139\n",
            "Run: 147, exploration: 0.010000000000000, score: 176\n",
            "Run: 148, exploration: 0.010000000000000, score: 178\n",
            "Run: 149, exploration: 0.010000000000000, score: 60\n",
            "Run: 150, exploration: 0.010000000000000, score: 28\n",
            "Run: 151, exploration: 0.010000000000000, score: 14\n",
            "Run: 152, exploration: 0.010000000000000, score: 47\n",
            "Run: 153, exploration: 0.010000000000000, score: 61\n",
            "Run: 154, exploration: 0.010000000000000, score: 25\n",
            "Run: 155, exploration: 0.010000000000000, score: 48\n",
            "Run: 156, exploration: 0.010000000000000, score: 106\n",
            "Run: 157, exploration: 0.010000000000000, score: 87\n",
            "Run: 158, exploration: 0.010000000000000, score: 14\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    cartpole()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of ML_gym_reinforcement_exercise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
